# Scope Intelligence - Backend & Pipeline

O **Scope Intelligence** Ã© a inteligÃªncia central do sistema de anÃ¡lise de chamados. Este backend Ã© responsÃ¡vel por processar grandes volumes de dados de suporte (chamados), identificar padrÃµes ocultos atravÃ©s de algoritmos de clustering avanÃ§ados e utilizar InteligÃªncia Artificial Generativa (LLM) para explicar o "porquÃª" desses problemas estarem ocorrendo.

Diferente de sistemas de BI tradicionais que mostram "o que" aconteceu, este sistema foca no "motivo", agrupando incidentes semanticamente similares e gerando insights qualitativos de forma automatizada.

## âš™ï¸ Tecnologias Utilizadas

O stack foi selecionado para lidar com processamento intensivo de dados (ETL), InteligÃªncia Artificial (Vetores + LLM) e alta disponibilidade via API.

- **[Python 3.10+](https://www.python.org/)**: Linguagem base para todo o processamento e API.
- **[FastAPI](https://fastapi.tiangolo.com/)**: Framework moderno e de alta performance para construÃ§Ã£o da API REST.
- **[Qdrant](https://qdrant.tech/)**: Banco de dados vetorial (Vector Database) utilizado para armazenar e buscar embeddings semÃ¢nticos dos chamados.
- **[OpenAI API](https://openai.com/)**: Utilizada em duas frentes:
  - **Embeddings (text-embedding-3-small)**: Para converter textos de chamados em vetores numÃ©ricos.
  - **Chat Completion (GPT-4o)**: Para analisar clusters, gerar tÃ­tulos, descriÃ§Ãµes e raciocÃ­nios tÃ©cnicos.
- **[HDBSCAN](https://hdbscan.readthedocs.io/)**: Algoritmo de clustering baseado em densidade hierÃ¡rquica, capaz de encontrar grupos de formatos variados e isolar ruÃ­dos (outliers).
- **[Scikit-Learn](https://scikit-learn.org/)**: Ferramentas auxiliares de ML (cÃ¡lculo de distÃ¢ncias, matrizes).
- **[SQLAlchemy](https://www.sqlalchemy.org/)**: ORM para comunicaÃ§Ã£o com o banco de dados relacional (MySQL).
- **[ReportLab](https://www.reportlab.com/)**: Biblioteca para geraÃ§Ã£o programÃ¡tica de relatÃ³rios em PDF.

## ğŸ“‘ TÃ³picos

- [1 - Estrutura do Projeto](#1-estrutura-do-projeto)
- [2 - Funcionalidades Principais](#2-funcionalidades-principais)
- [3 - Arquitetura e Fluxo do Pipeline](#3-arquitetura-e-fluxo-do-pipeline)
- [4 - InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#4-instalaÃ§Ã£o-e-configuraÃ§Ã£o)
- [5 - Como Executar](#5-como-executar)
- [6 - Modelagem de Dados](#6-modelagem-de-dados)

## <a id="1-estrutura-do-projeto"></a>1 - Estrutura do Projeto ğŸ—ï¸

A organizaÃ§Ã£o segue os princÃ­pios de separaÃ§Ã£o de responsabilidades, dividindo o cÃ³digo entre a API (online) e os Scripts de Pipeline (batch/offline).

```
â”œâ”€â”€ ğŸ“ app/                     # NÃºcleo da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ api/                 # DefiniÃ§Ã£o de Rotas e Schemas (Pydantic/FastAPI)
â”‚   â”œâ”€â”€ ğŸ“ core/                # ConfiguraÃ§Ãµes globais, ConexÃ£o DB e Vector Store
â”‚   â”œâ”€â”€ ğŸ“ models/              # Modelos de dados (se aplicÃ¡vel ao ORM)
â”‚   â””â”€â”€ ğŸ“ services/            # LÃ³gica de NegÃ³cio (O "CÃ©rebro" do sistema)
â”‚       â”œâ”€â”€ ğŸ cluster_engine.py # LÃ³gica de Clustering (HDBSCAN + Agrupamento HierÃ¡rquico)
â”‚       â”œâ”€â”€ ğŸ data_fetcher.py   # ETL: ExtraÃ§Ã£o de dados do MySQL
â”‚       â”œâ”€â”€ ğŸ llm_agent.py      # IntegraÃ§Ã£o com OpenAI (Prompts e Chamadas)
â”‚       â”œâ”€â”€ ğŸ vectorizer.py     # GeraÃ§Ã£o e gerenciamento de embeddings
â”‚       â””â”€â”€ ğŸ aggregator.py     # ConsolidaÃ§Ã£o estatÃ­stica de grupos
â”œâ”€â”€ ğŸ“ data_output/             # Armazenamento local dos JSONs gerados pelo pipeline
â”œâ”€â”€ ğŸ“ qdrant_data/             # PersistÃªncia de dados do container Qdrant
â”œâ”€â”€ ğŸ“ reports/                 # SaÃ­da dos relatÃ³rios PDF gerados
â”œâ”€â”€ ğŸ“ scripts/                 # Scripts de execuÃ§Ã£o Batch (Offline)
â”‚   â”œâ”€â”€ ğŸ run_pipeline.py      # Entrypoint principal da anÃ¡lise de dados
â”‚   â””â”€â”€ ğŸ generate_report.py   # Gerador de relatÃ³rios PDF a partir dos JSONs
â”œâ”€â”€ ğŸ“„ .env                     # VariÃ¡veis de ambiente e segredos
â”œâ”€â”€ ğŸ³ docker-compose.yml       # OrquestraÃ§Ã£o do serviÃ§o Qdrant
â”œâ”€â”€ ğŸ“„ ddl.sql                  # Schema do Banco de Dados Relacional
â””â”€â”€ ğŸ“„ requirements.txt         # DependÃªncias do Python
```

### ğŸ“¦ MÃ³dulos Principais
- **app/services/cluster_engine.py**: ContÃ©m a lÃ³gica matemÃ¡tica complexa. Recebe vetores brutos e retorna rÃ³tulos (labels) de agrupamento, alÃ©m de calcular a hierarquia (quais micro-clusters pertencem a quais macro-temas).
- **app/services/llm_agent.py**: Abstrai a "persona" do analista sÃªnior. ContÃ©m funÃ§Ãµes assÃ­ncronas para enviar contextos de clusters para o GPT e receber anÃ¡lises estruturadas (JSON mode).
- **scripts/run_pipeline.py**: O orquestrador. Ele nÃ£o contÃ©m lÃ³gica de negÃ³cio "pura", mas coordena a chamada sequencial de todos os serviÃ§os para transformar dados brutos no arquivo JSON final.

## <a id="2-funcionalidades-principais"></a>2 - Funcionalidades Principais ğŸš€

### 1. Pipeline de InteligÃªncia (Batch)
O coraÃ§Ã£o do sistema. Roda periodicamente (ou sob demanda) para varrer o banco de dados, vetorizar novos chamados e re-calcular os agrupamentos. O resultado Ã© um arquivo `.json` rico, contendo toda a Ã¡rvore de problemas detectados.

### 2. API REST (FastAPI)
ExpÃµe os dados processados para o Frontend.
- **Endpoints de Leitura**: Permitem que a interface carregue o JSON mais recente e exiba os grÃ¡ficos e cards.
- **Endpoints de Detalhe**: (Em desenvolvimento) Para drill-down de chamados especÃ­ficos.

### 3. GeraÃ§Ã£o de RelatÃ³rios PDF
Transforma a anÃ¡lise digital (JSON) em um documento executivo (`.pdf`). O relatÃ³rio inclui:
- Capa com resumo executivo.
- GrÃ¡ficos de tendÃªncia e distribuiÃ§Ã£o.
- Detalhamento dos top clusters (TÃ­tulo, explicaÃ§Ã£o, volumetria e exemplos).

## <a id="3-arquitetura-e-fluxo-do-pipeline"></a>3 - Arquitetura e Fluxo do Pipeline ğŸ§ 

O script `scripts/run_pipeline.py` executa um fluxo linear de 7 etapas crÃ­ticas. Entender esse fluxo Ã© essencial para manter o sistema.

### ETAPA 1: ExtraÃ§Ã£o e VetorizaÃ§Ã£o (ETL)
1.  **ConexÃ£o**: O sistema conecta no MySQL e busca chamados dos Ãºltimos X dias (ex: 180 dias) para o sistema alvo.
2.  **VetorizaÃ§Ã£o**:
    -   Cada chamado (TÃ­tulo + DescriÃ§Ã£o) Ã© enviado para a API de Embeddings da OpenAI.
    -   Recebemos um vetor de 1536 dimensÃµes representando o significado semÃ¢ntico do problema.
    -   Os vetores sÃ£o armazenados no **Qdrant** para cache (evitar gastar dinheiro re-processando chamados antigos).

### ETAPA 2: Clustering HierÃ¡rquico (MatemÃ¡tica)
Utilizamos uma abordagem hÃ­brida para agrupar os dados:
1.  **Micro-Clustering (HDBSCAN)**: O algoritmo analisa a densidade dos pontos no espaÃ§o vetorial. Pontos muito prÃ³ximos formam um "Micro-Cluster" (ex: "Erro de NullPointerException no Login"). Pontos isolados sÃ£o marcados como ruÃ­do (-1).
2.  **Macro-Agrupamento**: O sistema calcula o centrÃ³ide de cada micro-cluster e entÃ£o agrupa esses centrÃ³ides entre si, criando "Super Grupos" ou Categorias Pai (ex: "Falhas Gerais de AutenticaÃ§Ã£o"). Isso cria a Ã¡rvore de navegaÃ§Ã£o do sistema.

### ETAPA 3: AnÃ¡lise de Micro-Clusters (IA AssÃ­ncrona)
Para cada pequeno grupo formado:
1.  Selecionamos amostras representativas (chamados mais prÃ³ximos do centro do cluster).
2.  Enviamos para o LLM com um prompt especializado: *"Analise estes 10 chamados e defina um TÃ­tulo TÃ©cnico e uma DescriÃ§Ã£o do problema raiz."*
3.  O LLM retorna metadados estruturados (TÃ­tulo, Tags, AnÃ¡lise Racional).
> *Nota: Isso Ã© feito em paralelo (AsyncIO) para processar centenas de grupos em segundos.*

### ETAPA 4: ConsolidaÃ§Ã£o HierÃ¡rquica
O pipeline monta a estrutura de Ã¡rvore (Pai -> Filhos):
- Se um Pai tem vÃ¡rios filhos, ele agrega as mÃ©tricas de todos eles (soma volumes, combina top ofensores).
- Se um Pai tem apenas 1 filho, a estrutura Ã© "achatada" (Flatten) para simplificar a visualizaÃ§Ã£o.

### ETAPA 5: AnÃ¡lise Macro-Executiva (IA)
Para cada Categoria Pai formada:
1.  O sistema envia para o LLM os tÃ­tulos e descriÃ§Ãµes dos seus **Filhos**.
2.  O prompt muda: *"Atuando como um Gerente TÃ©cnico, resuma o que esses sub-problemas representam em alto nÃ­vel."*
3.  Isso gera os cards principais da dashboard.

### ETAPA 6: Tratamento de RuÃ­do
Chamados que nÃ£o formaram grupos densos (dispersos/variados) sÃ£o coletados em um grupo especial "Outros / Dispersos". Isso garante que 100% da volumetria seja contabilizada, mesmo que nÃ£o haja padrÃ£o claro.

### ETAPA 7: PersistÃªncia (JSON)
O resultado final Ã© salvo em `data_output/analise_<sistema>_<data>.json`. Este arquivo Ã© a fonte da verdade para o Frontend e para o gerador de PDF.

## <a id="4-instalaÃ§Ã£o-e-configuraÃ§Ã£o"></a>4 - InstalaÃ§Ã£o e ConfiguraÃ§Ã£o ğŸ› 

### PrÃ©-requisitos
1.  **Banco de Dados MySQL**: Contendo a tabela de chamados (ver `ddl.sql`).
2.  **Docker**: Para rodar o Qdrant.
3.  **Chave OpenAI**: NecessÃ¡ria para embeddings e LLM.

### ConfiguraÃ§Ã£o do Ambiente (.env)
Crie um arquivo `.env` na raiz baseado no exemplo abaixo:

```ini
# ConfiguraÃ§Ãµes do Projeto
PROJECT_NAME="Scope Intelligence"
DEBUG=True

# Banco de Dados (MySQL)
DATABASE_URL="mysql+mysqlconnector://user:pass@localhost:3306/suporte_db"

# Vetor Store (Qdrant)
QDRANT_HOST="localhost"
QDRANT_PORT=6333

# OpenAI (InteligÃªncia)
OPENAI_API_KEY="sk-..."
```

## <a id="5-como-executar"></a>5 - Como Executar â–¶ï¸

### 1. Subir Infraestrutura (Qdrant)
Inicie o banco vetorial utilizando Docker:
```powershell
docker-compose up -d
```

### 2. Executar o Pipeline de AnÃ¡lise
Para realizar uma anÃ¡lise completa de um sistema (ex: "PainelRH") considerando 180 dias de histÃ³rico:

```powershell
# Ative o ambiente virtual (recomendado)
.\venvscope\Scripts\activate

# Execute o script (a partir da raiz do projeto)
python scripts/run_pipeline.py --sistema "PainelRH" --dias 180
```
*Aguarde o processamento. Logs detalhados serÃ£o exibidos no terminal indicando cada etapa.*

### 3. Rodar a API (Opcional)
Se desejar servir os dados via API:
```powershell
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```
Acesse a documentaÃ§Ã£o automÃ¡tica em: `http://localhost:8000/docs`

### 4. Gerar RelatÃ³rio PDF
Para gerar o PDF a partir do Ãºltimo JSON gerado:
```powershell
python scripts/generate_report.py --input "data_output/analise_PainelRH_20231027.json"
```

## <a id="6-modelagem-de-dados"></a>6 - Modelagem de Dados ğŸ’¾

### Input (MySQL)
O sistema espera uma tabela com colunas mÃ­nimas para anÃ¡lise:
- `id_chamado` (Identificador Ãºnico)
- `titulo` (Texto curto)
- `descricao` (Texto longo - corpo do chamado)
- `data_abertura` (Datetime)
- `solicitante`, `area`, `status` (Metadados para filtros)

### Output (JSON Structure)
O JSON final possui o seguinte esquema simplificado:

```json
{
  "metadata": {
    "sistema": "PainelRH",
    "total_grupos": 12,
    "taxa_ruido": 0.15
  },
  "clusters": [
    {
      "cluster_id": 10001,
      "titulo": "Falha de AutenticaÃ§Ã£o SSO",
      "descricao": "Problemas relacionados a expiraÃ§Ã£o de token...",
      "metricas": { "volume": 150, "top_servicos": {...} },
      "sub_clusters": [
        { "titulo": "Erro 401 no Login", "volume": 80 },
        { "titulo": "Token InvÃ¡lido na API", "volume": 70 }
      ]
    }
  ]
}
```
